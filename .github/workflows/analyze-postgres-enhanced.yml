name: 3. Analyze Content (PostgreSQL Enhanced)

on:
  # Allow manual trigger
  workflow_dispatch:
    inputs:
      trigger_source:
        description: 'What triggered this workflow'
        required: false
        type: string
        default: 'manual'
      run_id:
        description: 'Previous workflow run ID'
        required: false
        type: string
      analysis_mode:
        description: 'Analysis mode'
        required: false
        type: choice
        default: 'recent'
        options:
          - recent        # Last 24 hours only
          - full          # Analyze everything
          - force         # Force re-analyze all
      use_enhanced:
        description: 'Use enhanced entity extraction'
        required: false
        type: boolean
        default: true
      cascade:
        description: 'Trigger next workflow in chain'
        required: false
        type: boolean
        default: true
  
  # Allow being called by other workflows
  workflow_call:
    inputs:
      trigger_source:
        description: 'What triggered this workflow'
        required: false
        type: string
        default: 'manual'
      run_id:
        description: 'Previous workflow run ID'
        required: false
        type: string
      analysis_mode:
        description: 'Analysis mode'
        required: false
        type: string
        default: 'recent'
      use_enhanced:
        description: 'Use enhanced entity extraction'
        required: false
        type: boolean
        default: true
      cascade:
        description: 'Trigger next workflow in chain'
        required: false
        type: boolean
        default: false

env:
  WORKFLOW_NAME: "3. Analyze Content (PostgreSQL Enhanced)"

permissions:
  contents: write
  actions: write

jobs:
  analyze:
    name: AI Analysis
    runs-on: ubuntu-latest
    timeout-minutes: 60
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'
          cache: 'npm'
          cache-dependency-path: github-actions-backend/package-lock.json

      - name: Install dependencies
        working-directory: github-actions-backend
        run: |
          echo "üì¶ Installing npm packages..."
          npm ci --prefer-offline --no-audit
          
      - name: Display workflow trigger info
        run: |
          echo "üîç PostgreSQL Analyze Workflow Information"
          echo "========================================"
          echo "Triggered by: ${{ inputs.trigger_source || 'unknown' }}"
          echo "Analysis Mode: ${{ inputs.analysis_mode || 'recent' }}"
          echo "Enhanced Mode: ${{ inputs.use_enhanced || true }}"
          echo "Previous Run ID: ${{ inputs.run_id || 'none' }}"
          echo "Cascade: ${{ inputs.cascade }}"
          echo "========================================"

      - name: Check PostgreSQL schema status (PROTECTED)
        working-directory: github-actions-backend
        env:
          POSTGRES_CONNECTION_STRING: ${{ secrets.POSTGRES_CONNECTION_STRING }}
          DATABASE_URL: ${{ secrets.POSTGRES_CONNECTION_STRING }}
        run: |
          echo "üîç Checking PostgreSQL schema status with protection..."
          
          # CRITICAL: Follow DATABASE PROTECTION rules
          # ‚úÖ Check schema status first
          if [ -f "schema-protector.js" ]; then
            echo "‚úÖ Schema protector found - checking status..."
            node schema-protector.js status || echo "‚ö†Ô∏è Schema status check had issues"
          else
            echo "‚ö†Ô∏è Schema protector not found - proceeding with caution"
          fi

      - name: Fix schema issues (PROTECTED EXECUTION)
        working-directory: github-actions-backend
        env:
          POSTGRES_CONNECTION_STRING: ${{ secrets.POSTGRES_CONNECTION_STRING }}
          DATABASE_URL: ${{ secrets.POSTGRES_CONNECTION_STRING }}
        run: |
          echo "üîß Running PROTECTED PostgreSQL schema fixes..."
          
          # CRITICAL: Follow DATABASE PROTECTION rules
          # ‚úÖ Use schema-protector.js wrapper
          # ‚úÖ Single script execution (no loops)
          # ‚úÖ Proper version control
          
          if [ -f "schema-protector.js" ] && [ -f "fix-enhanced-analysis-company-id.js" ]; then
            echo "üõ°Ô∏è  Using PROTECTED schema fix for enhanced_analysis..."
            node fix-enhanced-analysis-company-id.js
            echo "‚úÖ Protected schema fix completed"
          elif [ -f "fix-postgres-schema-complete.js" ]; then
            echo "‚ö†Ô∏è  Using legacy schema fix (not fully protected)..."
            node fix-postgres-schema-complete.js
            echo "‚úÖ Legacy schema fix completed"
          else
            echo "‚ùå Critical error: No schema fix scripts found!"
            echo "‚ùå Cannot proceed without fixing enhanced_analysis table"
            exit 1
          fi

      - name: Verify enhanced_analysis table structure
        working-directory: github-actions-backend
        env:
          POSTGRES_CONNECTION_STRING: ${{ secrets.POSTGRES_CONNECTION_STRING }}
          DATABASE_URL: ${{ secrets.POSTGRES_CONNECTION_STRING }}
        run: |
          echo "üîç Verifying enhanced_analysis table has required columns..."
          node -e "
            const { db, end } = require('./postgres-db');
            
            async function checkTable() {
              try {
                const columns = await db.all(\`
                  SELECT column_name, data_type, is_nullable
                  FROM information_schema.columns 
                  WHERE table_schema = 'intelligence' 
                  AND table_name = 'enhanced_analysis'
                  ORDER BY ordinal_position
                \`);
                
                if (columns.length === 0) {
                  console.log('‚ùå enhanced_analysis table does not exist!');
                  process.exit(1);
                }
                
                console.log('üìä enhanced_analysis table structure:');
                columns.forEach(col => {
                  console.log(\`  - \${col.column_name}: \${col.data_type} \${col.is_nullable === 'NO' ? 'NOT NULL' : 'NULL'}\`);
                });
                
                // Check for required columns
                const hasCompanyId = columns.some(col => col.column_name === 'company_id');
                const hasChangeId = columns.some(col => col.column_name === 'change_id');
                
                if (!hasCompanyId) {
                  console.log('‚ùå Missing required column: company_id');
                  process.exit(1);
                }
                
                if (!hasChangeId) {
                  console.log('‚ùå Missing required column: change_id');
                  process.exit(1);
                }
                
                console.log('‚úÖ All required columns found in enhanced_analysis table');
                console.log('üéâ Table is ready for AI analysis!');
                
              } catch (error) {
                console.error('‚ùå Error checking table structure:', error.message);
                process.exit(1);
              } finally {
                await end();
              }
            }
            
            checkTable();
          "

      - name: Verify baseline_analysis table structure  
        working-directory: github-actions-backend
        env:
          POSTGRES_CONNECTION_STRING: ${{ secrets.POSTGRES_CONNECTION_STRING }}
          DATABASE_URL: ${{ secrets.POSTGRES_CONNECTION_STRING }}
        run: |
          echo "üîç Verifying baseline_analysis table for enhanced entity extraction..."
          node -e "
            const { db, end } = require('./postgres-db');
            
            async function checkBaselineTable() {
              try {
                const columns = await db.all(\`
                  SELECT column_name, data_type
                  FROM information_schema.columns 
                  WHERE table_schema = 'intelligence' 
                  AND table_name = 'baseline_analysis'
                  ORDER BY ordinal_position
                \`);
                
                if (columns.length === 0) {
                  console.log('‚ùå baseline_analysis table does not exist!');
                  process.exit(1);
                }
                
                console.log('üìä baseline_analysis table columns:');
                columns.forEach(col => {
                  console.log(\`  - \${col.column_name}: \${col.data_type}\`);
                });
                
                // Check for JSONB columns needed for enhanced extraction
                const hasEntities = columns.some(col => col.column_name === 'entities' && col.data_type === 'jsonb');
                const hasThemes = columns.some(col => col.column_name === 'themes' && col.data_type === 'jsonb');
                
                if (hasEntities && hasThemes) {
                  console.log('‚úÖ Enhanced JSONB columns found - ready for rich entity extraction');
                } else {
                  console.log('‚ö†Ô∏è  Enhanced JSONB columns missing - will use basic text fields');
                  if (!hasEntities) console.log('  - Missing: entities JSONB');
                  if (!hasThemes) console.log('  - Missing: themes JSONB');
                }
                
              } catch (error) {
                console.error('‚ùå Error checking baseline_analysis table:', error.message);
                process.exit(1);
              } finally {
                await end();
              }
            }
            
            checkBaselineTable();
          "

      - name: üß† Run Baseline AI Analysis (PostgreSQL)
        if: success()
        working-directory: github-actions-backend
        env:
          GROQ_API_KEY: ${{ secrets.GROQ_API_KEY }}
          POSTGRES_CONNECTION_STRING: ${{ secrets.POSTGRES_CONNECTION_STRING }}
          DATABASE_URL: ${{ secrets.POSTGRES_CONNECTION_STRING }}
          NODE_ENV: production
        run: |
          echo "üß† Starting BASELINE AI Analysis (PostgreSQL)..."
          echo "üìä This analyzes the current state of all companies"
          
          # Choose analyzer based on use_enhanced flag
          if [ "${{ inputs.use_enhanced }}" = "true" ] || [ "${{ inputs.use_enhanced }}" = "" ]; then
            echo "üöÄ Using ENHANCED entity extraction analyzer"
            if [ -f "ai-analyzer-baseline-enhanced-postgres.js" ]; then
              if [ "${{ inputs.analysis_mode }}" == "force" ]; then
                echo "üîÑ Force mode: Re-analyzing all content"
                node ai-analyzer-baseline-enhanced-postgres.js --force
              else
                echo "‚ö° Smart mode: Only analyzing new content"
                node ai-analyzer-baseline-enhanced-postgres.js
              fi
            else
              echo "‚ö†Ô∏è Enhanced analyzer not found, falling back to standard"
              node ai-analyzer-baseline-three-db-postgres.js
            fi
          else
            echo "üìä Using standard analyzer"
            if [ "${{ inputs.analysis_mode }}" == "force" ]; then
              echo "üîÑ Force mode: Re-analyzing all content"
              node ai-analyzer-baseline-three-db-postgres.js --force
            else
              echo "‚ö° Smart mode: Only analyzing new content"
              node ai-analyzer-baseline-three-db-postgres.js
            fi
          fi
          
          echo "‚úÖ Baseline analysis complete"

      - name: üîç Run Change Detection Analysis (PostgreSQL)
        if: success()
        working-directory: github-actions-backend
        env:
          GROQ_API_KEY: ${{ secrets.GROQ_API_KEY }}
          POSTGRES_CONNECTION_STRING: ${{ secrets.POSTGRES_CONNECTION_STRING }}
          DATABASE_URL: ${{ secrets.POSTGRES_CONNECTION_STRING }}
          NODE_ENV: production
        run: |
          echo "üîç Starting CHANGE DETECTION AI Analysis (PostgreSQL)..."
          echo "üìä This analyzes what has changed recently"
          
          # Create error handling for ultra analyzer
          set +e  # Don't exit on error
          
          if [ "${{ inputs.analysis_mode }}" == "full" ] || [ "${{ inputs.analysis_mode }}" == "force" ]; then
            echo "üîÑ Full mode: Analyzing all changes"
            node ai-analyzer-ultra-three-db-postgres.js full
          else
            echo "‚ö° Recent mode: Only analyzing last 24 hours"
            node ai-analyzer-ultra-three-db-postgres.js recent
          fi
          
          ULTRA_EXIT_CODE=$?
          set -e  # Re-enable exit on error
          
          if [ $ULTRA_EXIT_CODE -ne 0 ]; then
            echo "‚ö†Ô∏è Ultra analyzer had issues (exit code: $ULTRA_EXIT_CODE)"
            echo "‚ö†Ô∏è This is often due to no recent changes to analyze"
            echo "‚ö†Ô∏è Continuing with workflow..."
          else
            echo "‚úÖ Change detection analysis complete"
          fi

      - name: üîÑ Generate change detail files (PostgreSQL)
        if: success()
        working-directory: github-actions-backend
        env:
          POSTGRES_CONNECTION_STRING: ${{ secrets.POSTGRES_CONNECTION_STRING }}
          DATABASE_URL: ${{ secrets.POSTGRES_CONNECTION_STRING }}
          NODE_ENV: production
        run: |
          echo "üìù Generating individual change detail files..."
          
          # Check if the PostgreSQL version exists
          if [ -f "generate-change-details-postgres.js" ]; then
            node generate-change-details-postgres.js
            echo "‚úÖ Change details generated"
          else
            echo "‚ö†Ô∏è PostgreSQL change details generator not found - skipping"
          fi

      - name: üíæ Commit analysis results
        if: success()
        env:
          GH_TOKEN: ${{ secrets.AI_COMPETITIVE_MONITOR_TOKEN }}
        run: |
          # Configure git
          git config --global user.name "GitHub Action"
          git config --global user.email "action@github.com"
          
          # Function to push with retry logic
          push_with_retry() {
            local max_attempts=5
            local attempt=1
            local wait_time=1
            
            while [ $attempt -le $max_attempts ]; do
              echo "üîÑ Attempt $attempt/$max_attempts to push changes..."
              
              # Pull latest changes before each attempt
              git pull origin main --rebase || {
                echo "‚ùå Failed to rebase, trying merge strategy..."
                git rebase --abort 2>/dev/null || true
                git pull origin main --no-rebase --strategy=ours || {
                  echo "‚ùå Merge failed, resetting to origin/main"
                  git reset --hard origin/main
                  return 1
                }
              }
              
              # Try to push
              if git push origin main; then
                echo "‚úÖ Successfully pushed changes!"
                return 0
              else
                echo "‚ùå Push failed"
                
                if [ $attempt -lt $max_attempts ]; then
                  echo "‚è≥ Waiting ${wait_time}s before retry..."
                  sleep $wait_time
                  wait_time=$((wait_time * 2))
                  attempt=$((attempt + 1))
                else
                  echo "‚ùå All push attempts failed"
                  return 1
                fi
              fi
            done
          }
          
          # Add all analysis outputs
          git add api-data/ || true
          git add github-actions-backend/data/*.json || true
          
          if git diff --staged --quiet; then
            echo "No changes to commit"
          else
            echo "üìù Changes to commit:"
            git diff --staged --name-only
            
            COMMIT_MSG="üöÄ Update PostgreSQL AI analysis"
            if [ "${{ inputs.use_enhanced }}" = "true" ] || [ "${{ inputs.use_enhanced }}" = "" ]; then
              COMMIT_MSG="$COMMIT_MSG (Enhanced Entities)"
            fi
            COMMIT_MSG="$COMMIT_MSG - $(date -u +%Y-%m-%d_%H:%M:%S_UTC)"
            
            git commit -m "$COMMIT_MSG"
            
            # Push changes with retry logic
            push_with_retry || {
              echo "‚ö†Ô∏è Failed to push changes after multiple attempts"
              echo "‚ö†Ô∏è This is likely due to concurrent workflows"
              echo "‚ö†Ô∏è Changes will be included in next workflow run"
              # Don't fail the workflow - changes are committed locally
              # and will be pushed in the next successful run
              exit 0
            }
          fi
      
      - name: Trigger Sync workflow (if configured)
        if: inputs.cascade == true
        uses: actions/github-script@v7
        with:
          github-token: ${{ secrets.AI_COMPETITIVE_MONITOR_TOKEN }}
          script: |
            try {
              // Check if sync-postgres.yml exists
              const { data: files } = await github.rest.repos.getContent({
                owner: context.repo.owner,
                repo: context.repo.repo,
                path: '.github/workflows'
              });
              
              const syncWorkflowExists = files.some(file => 
                file.name === 'sync-postgres.yml'
              );
              
              if (syncWorkflowExists) {
                const result = await github.rest.actions.createWorkflowDispatch({
                  owner: context.repo.owner,
                  repo: context.repo.repo,
                  workflow_id: 'sync-postgres.yml',
                  ref: 'main',
                  inputs: {
                    trigger_source: 'analyze-postgres-cascade',
                    run_id: '${{ github.run_id }}'
                  }
                });
                console.log(`‚úÖ Triggered PostgreSQL Sync workflow`);
              } else {
                console.log(`‚ö†Ô∏è  sync-postgres.yml not found - skipping cascade`);
                console.log(`   This is normal - full-monitor-postgres.yml handles sync inline`);
              }
            } catch (error) {
              console.log(`‚ö†Ô∏è  Could not trigger sync workflow:`, error.message);
              console.log(`   This is normal if sync-postgres.yml doesn't exist`);
            }
